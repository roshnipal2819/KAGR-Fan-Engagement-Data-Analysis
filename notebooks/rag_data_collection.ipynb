{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY in your .env file.\")\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Web Scraping to Collect External Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data from https://www.espn.com/nba/team/_/name/bos/boston-celtics...\n",
      "Collected 0 articles.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to collect data from a sports news website\n",
    "def get_sports_data_from_web(url):\n",
    "    print(f\"Collecting data from {url}...\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract information such as article titles, summaries, and publication dates\n",
    "        articles = []\n",
    "        \n",
    "        # Finding the latest news section\n",
    "        news_section = soup.find('section', attrs={'data-name': 'news-feed'})\n",
    "        if news_section:\n",
    "            for item in news_section.find_all('div', class_='ContentList__Item'):\n",
    "                title = item.find('h1') or item.find('h2') or item.find('h3')\n",
    "                summary = item.find('p')\n",
    "                date = item.find('time')['datetime'] if item.find('time') else datetime.now().isoformat()\n",
    "                \n",
    "                title_text = title.get_text(strip=True) if title else \"\"\n",
    "                summary_text = summary.get_text(strip=True) if summary else \"\"\n",
    "                \n",
    "                if title_text:\n",
    "                    articles.append({\n",
    "                        'title': title_text,\n",
    "                        'summary': summary_text,\n",
    "                        'publication_date': date\n",
    "                    })\n",
    "        \n",
    "        print(f\"Collected {len(articles)} articles.\")\n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Example URL for the Boston Celtics team page\n",
    "sports_url = 'https://www.espn.com/nba/team/_/name/bos/boston-celtics'\n",
    "external_articles = get_sports_data_from_web(sports_url)\n",
    "\n",
    "# Save the articles to a CSV file for further analysis\n",
    "if external_articles:\n",
    "    external_articles_df = pd.DataFrame(external_articles)\n",
    "    external_data_path = 'data/external/sports_articles_data.csv'\n",
    "    external_articles_df.to_csv(external_data_path, index=False)\n",
    "    print(f\"External articles data saved to {external_data_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example URL (replace with a valid URL for data collection)\n",
    "# sports_url = 'https://www.espn.com/nba/team/_/name/bos/boston-celtics'\n",
    "# external_articles = get_sports_data_from_web(sports_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/roshni/Documents/KAGR-Project/Project/Fan_engagement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roshni/Documents/KAGR-Project/Project/Fan_engagement/.venv/lib/python3.13/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External articles data saved to data/external/sports_articles_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save the articles to a CSV file for further analysis\n",
    "external_data_path = 'data/external/sports_articles_data.csv'\n",
    "external_articles_df = pd.DataFrame(external_articles)\n",
    "external_articles_df.to_csv(external_data_path, index=False)\n",
    "print(f\"External articles data saved to {external_data_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to use GPT-4 to summarize articles and extract insights\n",
    "def generate_summary_with_openai(article_list, max_tokens=100):\n",
    "    summaries = []\n",
    "    for article in article_list:\n",
    "        prompt = (\n",
    "            f\"Summarize the following sports article in a concise manner and provide key takeaways:\\n\"\n",
    "            f\"Title: {article['title']}\\n\"\n",
    "            f\"Summary: {article['summary']}\\n\"\n",
    "        )\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "                engine=\"text-davinci-003\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            generated_summary = response['choices'][0]['text'].strip()\n",
    "            summaries.append({\n",
    "                'title': article['title'],\n",
    "                'original_summary': article['summary'],\n",
    "                'generated_summary': generated_summary,\n",
    "                'publication_date': article['publication_date']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary for article '{article['title']}': {e}\")\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for summarization.\n"
     ]
    }
   ],
   "source": [
    "# Use OpenAI to generate summaries of the collected articles\n",
    "if external_articles:\n",
    "    augmented_data = generate_summary_with_openai(external_articles)\n",
    "    augmented_data_path = '../data/external/augmented_sports_articles_data.csv'\n",
    "    augmented_data_df = pd.DataFrame(augmented_data)\n",
    "    augmented_data_df.to_csv(augmented_data_path, index=False)\n",
    "    print(f\"Augmented sports articles data saved to {augmented_data_path}.\")\n",
    "else:\n",
    "    print(\"No articles found for summarization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augmented_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(publication_counts)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Analyze the augmented data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m analyze_augmented_data(\u001b[43maugmented_data_path\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG Data Collection Completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'augmented_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "### Step 3: Extracting Key Insights and Saving for Further Use\n",
    "\n",
    "# Load the augmented data to analyze the insights\n",
    "def analyze_augmented_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Example Analysis: Counting articles by publication date\n",
    "    print(\"Analyzing the augmented sports articles data...\")\n",
    "    publication_counts = df['publication_date'].apply(lambda x: x.split('T')[0]).value_counts()\n",
    "    print(\"Publication counts by date:\")\n",
    "    print(publication_counts)\n",
    "\n",
    "# Analyze the augmented data\n",
    "analyze_augmented_data(augmented_data_path)\n",
    "\n",
    "print(\"RAG Data Collection Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
